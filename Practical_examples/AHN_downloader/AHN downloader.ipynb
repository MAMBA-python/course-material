{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and processing AHNx\n",
    "\n",
    "developed by Teije van der Horst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will automatically download the 'Actueel Hoogtebestand nederland' (AHN). The example is set for the AHN2 5m which is easier on the internet connection and processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this script, we make use of the feedparser package **unless you have Python 3.7 in which case you have to install requests** \n",
    "\n",
    "feedparser can be installed through the command line using:\n",
    "\n",
    "```console\n",
    "$ pip install feedparser\n",
    "```\n",
    "\n",
    "and requests (if not installed yet) through\n",
    "\n",
    "```console\n",
    "$ conda install -c conda-forge requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfileobj\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AHN2\n",
    "First we need to specify which version of the AHN we want to download. There are some choices available (see https://www.pdok.nl/downloads?articleid=1948857#1066961ba7c918cc4245e7b4f66efd7f for a detailed explanation)\n",
    "\n",
    "- AHN2 0.5 meter maaiveld raster, opgevuld: \n",
    "http://geodata.nationaalgeoregister.nl/ahn2/atom/ahn2_05m_int.xml\n",
    "\n",
    "- AHN2 0.5 meter maaiveld raster, niet opgevuld: \n",
    "http://geodata.nationaalgeoregister.nl/ahn2/atom/ahn2_05m_non.xml\n",
    "\n",
    "- AHN2 0.5 meter ruw raster:\n",
    "http://geodata.nationaalgeoregister.nl/ahn2/atom/ahn2_05m_ruw.xml\n",
    "\n",
    "- AHN2 5 meter maaiveld raster:\n",
    "http://geodata.nationaalgeoregister.nl/ahn2/atom/ahn2_5m.xml\n",
    "\n",
    "We will use the latter product in this example.\n",
    "\n",
    "## AHN3\n",
    "or AHN3, it seems that the RSS feed (xml link) is not available (anymore/yet). For this, it is possible to construct a download link using a recipe:\n",
    "1. base_url = `'https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_'`\n",
    "2. raster_type = \n",
    "    - `'05m_dsm/R_'` (0,5 meter raster dsm = oppervlakte metingen)\n",
    "    - `'05m_dtm/M_'` (0,5 meter raster dtm = maaiveld)\n",
    "    - `'5m_dsm/R5_'` (5 meter raster dsm = oppervlakte metingen)\n",
    "    - `'5m_dtm/M5_'` (5 meter raster dtm = maaiveld)\n",
    "    - `'laz/C_'` (puntenwolk)\n",
    "3. mapindex = `'38FZ2'` (for example) **NOTE: the 'ahn_units.shp' bladindex is used for AHN3 0.5 and 5 m and indices need to be converted to upper case**\n",
    "4. extension = \n",
    "    - `'.ZIP'`\n",
    "    - `'.LAZ'`\n",
    "\n",
    "`url = base_url + raster_type + mapindex + extension`\n",
    "https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz/C_38GN2.LAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = 'https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_'\n",
    "# raster_type = '05m_dsm/R_'\n",
    "# mapindex = '31hn2'.upper()\n",
    "# extension = '.ZIP'\n",
    "# url = base_url + raster_type + mapindex + extension\n",
    "# print(url)  \n",
    "# should yield: https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_05m_dsm/R_31HN2.ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahn2rss = 'http://geodata.nationaalgeoregister.nl/ahn2/atom/ahn2_5m.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find a way to download a file. For this we are going to use the `requests` package which a very nice one and which is available on all python versions (officially supports Python 2.7 & 3.4â€“3.7) and operating systems (which is not the case for the buitin scripts for downloading that have changed slightly over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statuscode 200 indicates successful request\n",
      "\tstatus_code = 200\n",
      "\n",
      "{'Date': 'Tue, 15 Jan 2019 18:33:39 GMT', 'Content-Type': 'text/xml', 'Content-Length': '1157246', 'X-Cnection': 'close, close', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'POST, GET, OPTIONS, HEAD', 'Access-Control-Max-Age': '1000', 'Access-Control-Allow-Headers': 'SOAPAction,X-Requested-With,Content-Type,Origin,Authorization,Accept'}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(ahn2rss)\n",
    "print('Statuscode 200 indicates successful request')\n",
    "print('\\tstatus_code = {}\\n'.format(r.status_code))\n",
    "print(r.headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the request instance `r`, we can download the content that we have requested. Since we will be using this more often, it is nice to have this operation in a function. For this we can use the `iter_content` method which chunks up the requested file into managable pieces (given that streaming is supported for this request)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, out_path, fn):\n",
    "    r = requests.get(url, verify=True, stream=True)\n",
    "    if r.status_code == 200:  # 200 means successfull get request\n",
    "        if not os.path.isdir(out_path): \n",
    "            os.mkdir(out_path)  # This works only if one folder layer needs to be created\n",
    "        fn = os.path.join(out_path, fn)  # combine path and filename, this works on all operating systems\n",
    "        with open(fn, 'wb') as f:  # Open a file and close after the while statement\n",
    "            for chunk in r.iter_content(1024*1024):  # read 1 MB at a time\n",
    "                f.write(chunk)\n",
    "        return fn\n",
    "    else:\n",
    "        raise RuntimeWarning('Could not download from {}\\n status_code = {}'.format(url, r.status_code))\n",
    "        return r.status_code\n",
    "    # No return statement necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"note\"></div>       \n",
    "**NOTE**:  For Python 3.7, the feedparser breaks, so we need another method to get the desired output. We parse the RSS xml using requests, and then we read that using the xml package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "tiles = {}\n",
    "if sys.version.startswith('3.7'):\n",
    "    \n",
    "    xml = download(ahn2rss, out_path='AHN_XML', fn='ahn2_5m.xml')\n",
    "    \n",
    "    import xml.etree.ElementTree as ET\n",
    "    \n",
    "    tree = ET.parse(xml)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    prefix = '{http://www.w3.org/2005/Atom}'  # parsing has prefix for some reason?\n",
    "    for entry in root.findall(prefix+'entry'):\n",
    "        tile_id = entry.find(prefix+'id').text.split('.tif.zip')[0].split('_')[-1]  # Change this line for 0.5 meter\n",
    "        tile_link = entry.find(prefix+'link').attrib['href']\n",
    "        tiles[tile_id] = tile_link\n",
    "            \n",
    "else:\n",
    "    import feedparser\n",
    "    d = feedparser.parse(ahn2rss)\n",
    "    entries = d['entries']\n",
    "    links = [e['links'][0]['href'].encode('ascii') for e in entries]\n",
    "    for link in links:\n",
    "        tiles[link.split('/')[-1]] = link \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should now have a Python dictionary with all available tiles and the adress to get them from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile '01cz1' can be found through http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_01cz1.tif.zip\n",
      "Tile '01cz2' can be found through http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_01cz2.tif.zip\n",
      "Tile '01dz1' can be found through http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_01dz1.tif.zip\n",
      "Tile '01dz2' can be found through http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_01dz2.tif.zip\n",
      "Tile '01gn1' can be found through http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_01gn1.tif.zip\n",
      "...\n",
      "And so on ...\n"
     ]
    }
   ],
   "source": [
    "for key in list(tiles.keys())[0:5]:\n",
    "    print(\"Tile '{}' can be found through {}\".format(key, tiles[key]))\n",
    "print('...\\nAnd so on ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now we have to filter which of those to download by means of the 'bladindex' shapefiles (see http://www.ahn.nl/common-nlm/open-data.html). These can be found at the following two adresses for the **main units (5m products)**:\n",
    "- http://www.ahn.nl/binaries/content/assets/ahn-nl/downloads/ahn_units.zip\n",
    "\n",
    "and also for the **sub units (0.5 m products)**\n",
    "- http://www.ahn.nl/binaries/content/assets/ahn-nl/downloads/ahn_subunits.zip\n",
    "\n",
    "Let's download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bladindex1 = 'http://www.ahn.nl/binaries/content/assets/ahn-nl/downloads/ahn_units.zip'\n",
    "ahn_units = download(bladindex1, 'bladindex', 'ahn_units.zip')\n",
    "bladindex2 = 'http://www.ahn.nl/binaries/content/assets/ahn-nl/downloads/ahn_subunits.zip'\n",
    "ahn_subunits = download(bladindex2, 'bladindex', 'ahn_subunits.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't access the zipfile directly, so we have to unzip it. Let's make a function for that so we don't have to do this more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "def unzip(zipfile, out_path, delete=True):\n",
    "    if not os.path.isdir(out_path): \n",
    "            os.mkdir(out_path)\n",
    "    with ZipFile(zipfile, 'r') as zipobj:\n",
    "        zipobj.extractall(out_path)\n",
    "    if delete:\n",
    "        os.remove(zipfile)\n",
    "    # No return statement necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ofcourse, now we use the function to extract the zipfile we just downloaded, next to the zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip(ahn_units, 'bladindex', delete=False)\n",
    "unzip(ahn_subunits, 'bladindex', delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to filter the required tiles (if necessary) by using for instance another shapefile to intersect the 'bladindex' with. There are various ways of doing this, some of which are explained in:\n",
    "https://gis.stackexchange.com/questions/178765/intersecting-two-shapefiles-from-python-or-command-line\n",
    "\n",
    "This method uses geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bladindex\\ahn_units.shp\n",
      "shapefile\\ahn_intersect.shp\n"
     ]
    }
   ],
   "source": [
    "blad_shp = ahn_units.split('.zip')[0] + '.shp'  # Gets the shapefile filename\n",
    "isct_shp = os.path.join('shapefile', 'ahn_intersect.shp')\n",
    "print(blad_shp)\n",
    "print(isct_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "g1 = gpd.GeoDataFrame.from_file(blad_shp)\n",
    "g2 = gpd.GeoDataFrame.from_file(isct_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300\" height=\"300\" viewBox=\"138621.3824380594 455262.51716339664 2985.1361558327335 2640.7830453471397\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,913165.8173721405)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"19.900907705551557\" opacity=\"0.6\" d=\"M 138731.9430364236,457723.34072917746 L 138984.42226029813,457792.7396103796 L 139193.86085514704,457242.00279180903 L 139520.8248885218,456731.816568756 L 139919.14257453106,456508.18646149547 L 140141.9087843788,456437.28059328836 L 140570.33112644788,456459.43227800564 L 140998.74951021752,456481.6207518829 L 141432.4893521933,456304.82737834734 L 141495.95799552795,455906.6228199262 L 141331.174126762,455737.3139132552 L 140920.56426332754,455808.70379741787 L 140503.89300749978,455809.8995853007 L 140292.32911717257,455711.0050862326 L 140033.4540973463,455495.1781910314 L 139757.23565756177,455373.0777617608 L 139505.04549740884,455432.39872549777 L 139229.84895276855,455638.15619589086 L 139096.15668736072,456042.4896234183 L 139108.60347144314,456264.8780988137 L 139045.0865964253,456586.99937055266 L 138824.05465842655,457184.66979262 L 138731.9430364236,457723.34072917746 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x1c5fee8d438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo2 = next(g2.itertuples()).geometry\n",
    "geo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding \"31hn2\" to download list\n",
      "Adding \"32cn1\" to download list\n",
      "Adding \"31hz2\" to download list\n",
      "Adding \"32cz1\" to download list\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "for blad in g1.itertuples():\n",
    "    if geo2.intersects(blad.geometry):\n",
    "        selection.append(blad.UNIT)\n",
    "        print('Adding \"{}\" to download list'.format(selection[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['31hn2', '32cn1', '31hz2', '32cz1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'31hn2': 'http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_31hn2.tif.zip', '32cn1': 'http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_32cn1.tif.zip', '31hz2': 'http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_31hz2.tif.zip', '32cz1': 'http://geodata.nationaalgeoregister.nl/ahn2/extract/ahn2_5m/ahn2_5_32cz1.tif.zip'}\n"
     ]
    }
   ],
   "source": [
    "# Filter the bladindex shapefiles using a costum made shapefile\n",
    "selected_tiles = {key:tiles[key] for key in selection}\n",
    "print(selected_tiles)\n",
    "\n",
    "# Uncomment the next line to get tile 600-604 from the tiles dict as an example\n",
    "# selected_tiles = {key:value for key, value in zip(list(tiles.keys())[600:605], list(tiles.values())[600:605])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered dictionary is ready for download now. Let's create an output folder to download our tiff files to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipdir = 'AHN2_5m_zip'\n",
    "if not os.path.isdir(zipdir):  # Always check if a folder was already created first\n",
    "    os.mkdir(zipdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the actual files, if they don't exist yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to: AHN2_5m_zip\\ahn2_5_31hn2.tif.zip\n",
      "Downloading to: AHN2_5m_zip\\ahn2_5_32cn1.tif.zip\n",
      "Downloading to: AHN2_5m_zip\\ahn2_5_31hz2.tif.zip\n",
      "Downloading to: AHN2_5m_zip\\ahn2_5_32cz1.tif.zip\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "downloaded = []\n",
    "for tile in selected_tiles.keys():\n",
    "    fn = selected_tiles[tile].split('/')[-1]\n",
    "    ziptile = os.path.join(zipdir, fn)\n",
    "    if os.path.exists(ziptile) and os.path.getsize(ziptile) > 0:\n",
    "        print('File \"{}\" already downloaded, skipping'.format(ziptile))\n",
    "        downloaded.append(ziptile)\n",
    "    else:\n",
    "        print('Downloading to: {}'.format(ziptile))\n",
    "        output = download(selected_tiles[tile], zipdir, fn)\n",
    "        downloaded.append(output)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now unzip these files into a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifdir = 'AHN2_5m_tif'\n",
    "for zipfile in downloaded:\n",
    "    unzip(zipfile, tifdir, delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have downloaded these files already, we can combine them into one file, WITHOUT loading the data into memory. For this we use the incredible Geospatial Data Abstraction Library ([gdal](https://gdal.org/)) package. \n",
    "\n",
    "If you don't have gdal yet, install it in your conda (!) environment using\n",
    "```console\n",
    "$ conda install -c conda-forge gdal\n",
    "```\n",
    "\n",
    "Notice that it is loaded with the command: \n",
    "```python\n",
    ">>> from osgeo import gdal```\n",
    "\n",
    "gdal is what drives QGIS in the background, and is a very powerful and well maintained library that supports nearly all of the common gridded dataset IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to make a virtual raster dataset, also known as a [vrt](https://gdal.org/gdal_vrttut.html) which is a very useful way of combining rasters without having to compute a thing. We need this vrt for further calculations, but even a QGIS browser supports loading this vrt (try it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "tifs = glob.glob(tifdir + os.path.sep + '*.tif')  # extract all tif files\n",
    "ds = gdal.BuildVRT('.\\AHN2_5m.vrt', tifs)  # build a vrt file\n",
    "ds.FlushCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to cut out a certain area from the AHN using the shapefile that we used to intersect with the bladindex. For this, we can still use gdal. The gdalwarp can be used to convert a dataset to another using a cutline (and a ton of other options as well). For now we stick to the cutline option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_raster = 'AHN2_5m_cut.tif'\n",
    "clip = gdal.Warp(output_raster, ds,\n",
    "                 options=gdal.WarpOptions(format='GTiff',\n",
    "                                          dstSRS=ds.GetProjectionRef(),\n",
    "                                          cutlineDSName=isct_shp,\n",
    "                                          cropToCutline=True,\n",
    "                                          creationOptions= [ 'COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=IF_SAFER']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we're done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working with *puntenwolken*, then you are downloading LAZ files. These files are very large and might not fit into memory when reading them. Have a look at some Python packages that are created for working with LIDAR LAZ files (advanced):\n",
    "- [pydal](https://pdal.io/)\n",
    "- [lazpy](https://pypi.org/project/laspy/)\n",
    "- [PyLidar](http://www.pylidar.org/en/latest/)\n",
    "- [~~liblas~~](https://liblas.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
