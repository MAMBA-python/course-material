{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "   <IMG SRC=\"https://mamba-python.nl/images/logo_basis.png\" WIDTH=125 ALIGN=\"right\">\n",
    "   \n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "#  Oefening voor webscrapen met BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "Webscraping is een techniek om automatisch toegang te krijgen tot grote hoeveelheden informatie van een website, zonder handmatig alles te moeten bezoeken. Webscraping kan aan het begin een beetje intimiderend zijn, daarom gaan we het in dit werkboek stap voor stap doorlopen. \n",
    "\n",
    "We gebruiken package Beautiful soup, requests en urllib.requests omdat dit relatief eenvoudig te gebruiken is. Voor geavanceerdere toepassingen van webscrapen is het package scrapy geschikter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### De stappen om webpagina's binnen te halen\n",
    "\n",
    "In dit voorbeeld bekijken we eerst hoe we dit zouden doen voor de website 'http://web.mta.info/developers/turnstile.html. Deze bevat data informatie over de metro's in New York per station en lijn. Deze bestanden zijn te downloaden per dag. Zonder scrapen zou dit betekenen dat al deze bestanden één voor één geopend moeten worden en opgeslagen. Gelukkig kan dat met webscrapen handiger. Hoe dit moet is in de volgende stappen beschreven. Daaronder staat een oefening om het zelf te proberen.\n",
    "\n",
    "<br/>\n",
    "<figure>\n",
    "   <IMG SRC=\"imgs/Turnstile_img.png\" WIDTH=725 ALIGN=\"middle\">\n",
    "   \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importeer de packages\n",
    "\n",
    "Beautiful soup werkt altijd samen met andere packages om de requests naar de browser te sturen.  \n",
    "Daarvoor importeren we requests en urllib.requests. importeer lxml om de html goed te kunnen parsen. Importeer time om de nodige pauze in het scrapen te kunnen aanbrengen. \n",
    "\n",
    "Deze kunnen geinstalleeerd worden via 'pip install' of 'conda install' in je Anaconda Prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set the URL you want to webscrape from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://web.mta.info/developers/turnstile.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Connect to the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response # het getal in haakjes geeft aan wat de status van het request is. 200 betekend gelukt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parse de HTML met Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier wordt de html van de response binnen gehaald en geparst aan de hand van het lxml package.\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doorlinken naar subpagina's\n",
    "\n",
    "Dat valt best mee. Nu is het vervolgens de kunst om de goede informatie uit de html te halen. Hiervoor is het handig om de basis van HTML te kennen. Voor meer informatie over HTML kun je hier kijken: https://www.w3schools.com/html/html_intro.asp. Om de rauwe text achter de website te zien ga je naar de website, klik je op de rechter muisknop en dan op 'inspect'/'Element controleren'.\n",
    "<br/>\n",
    "<br/>\n",
    "<figure>\n",
    "   <IMG SRC=\"/imgs/inspect.png\" WIDTH=180 ALIGN=\"middle\">\n",
    "   \n",
    "</figure>\n",
    "\n",
    "\n",
    "De HTML verschijnt dan aan de rechterkant (Chrome) of onderkant (Internet Explorerer) van je browser. Het zou er zo uit moeten zien:\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "<figure>\n",
    "   <IMG SRC=\"imgs/inspect_output.png\" WIDTH=725 ALIGN=\"middle\">\n",
    "   \n",
    "</figure>\n",
    "<br/><br/>\n",
    "Als je met de rechtermuisknop klinkt op een specifiek element (bijvoorbeeld de titel) en op inspecteren klikt, zie je gelijk waar dat stukje in het html bestand wordt aangegeven. \n",
    "\n",
    "De HTML pagina is gebouwd aan de hand van 'tags'. Zo'n tag vertelt de browser om iets te doen. Tags staan altijd binnen '<' en '>'.  In het voorbeeld hieronder is <html> een tag. De tag wordt afgesloten door de naam te herhalen met een '/' ervoor: </html>\n",
    "    \n",
    "<br/>\n",
    " <figure>\n",
    "   <IMG SRC=\"imgs/html.png\" WIDTH=325 ALIGN=\"middle\">\n",
    "   \n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We kunnen de tags opvragen aan de hand van hun naam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pak een specifieke tag uit je lijst\n",
    "\n",
    "De eerste datafile die we willen bekijken start op regel 36. We kunnen deze opvragen uit de lijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_a_tag = soup.findAll('a')[36]\n",
    "one_a_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We willen de echte link eruit halen\n",
    "\n",
    "we kunnen naar de attribute 'href' uit de tag met label a verwijzen met vierkante haken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = one_a_tag['href']\n",
    "link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Het pad is relatief: \n",
    "de volledige download is:  'http://web.mta.info/developers/' + link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_url  = 'http://web.mta.info/developers/'+ link\n",
    "\n",
    "# Om te downloaden gebruiken we request.urlretrieve + de download link. Deze zou in het mapje moeten komen te staan. \n",
    "urllib.request.urlretrieve(tot_url) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 1: download de eerste 5 files\n",
    "\n",
    "Als we meerdere requests doen naar een server kunnen we deze overbelasten. Het is daarom netjes om een pauze in je request te bouwen. Bovendien voorkomt dit dat je onnodig geblokkeerd wordt. Hiervoor kun je het package time gebruiken.\n",
    "\n",
    "time.sleep(1) laat bijvoorbeeld je script een seconde pauzeren. \n",
    "\n",
    "Opdracht: Download de eerste 5 files van de mta website met een ingebouwde pauze van 2 seconde tussen de downloads in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#antw1\">Antwoord Oefening 1</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oefeningen met quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De onderstaande oefeningen zijn om te oefenen met het binnenhalen van specifieke data. We gebruiken hiervoor de website 'http://quotes.toscrape.com/'. Dit is een oefen-website voor scrapen waarom quotes staan van verschillende schrijvers uit GoodReads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 2:\n",
    "\n",
    "Lees met beautiful soup de data (geparst) binnen van 'http://quotes.toscrape.com/'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#antw2\">Antwoord Oefening 2</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificeren van tags en attributes\n",
    "\n",
    "De functie findall & find kunnen we niet alleen gebruiken om tag namen te vinden, maar ook welke waarde de attirbutes moeten hebben, of dat we alleen de textuele waarden willen zien.  Ter illustratie:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vb 1 Finding a text based on attributes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### vb1 de eerste quote heeft als tag:\n",
    "# <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. \n",
    "# It cannot be changed without changing our thinking.”</span>\n",
    "\n",
    "# Haal de goede website binnen (als het goed is al gedaan)\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "\n",
    "# we kunnen de attributes class en itemprop en de waarde die hebben opgeven. .text geeft dan de text die tussen de <span> ...\n",
    "# en </span> instaat. In dit geval de eerste quote\n",
    "soup.find('span', attrs={'class': 'text', 'itemprop':'text'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vb2 Finding all text based on attributes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dit kunnen we ook gebruiken met findall:\n",
    "\n",
    "all_quotes_elements = soup.find_all('span', attrs={'class': 'text', 'itemprop':'text'})\n",
    "\n",
    "# we kunnen nu niet de .text opvragen omdat we een hele set met resultaten hebben gekregen, van het type 'resultset'\n",
    "print(type(all_quotes_elements))\n",
    "\n",
    "# Dit ziet er zo uit\n",
    "print(all_quotes_elements[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als we hier doorheen loopen dan kunnen we een voor een de quote opvragen\n",
    "for quote in all_quotes_elements:\n",
    "    print(quote.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vb 3 Find attribute values based on tag name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Om de attribute value terug te krijgen gebruiken we weer [attribute_name] nadat we het element \n",
    "# hebben gevonden aan de hand van de tag naam. \n",
    "\n",
    "# Laten we de links zoeken naar de author description. Met inspecteren zien we dat de tag er zo uit ziet\n",
    "# <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "# Echter als we zoeken op alleen tag-naam a dan vinden we veel meer tags dan alleen die met de href die wij zoeken\n",
    "\n",
    "for element in soup.find_all(\"a\"):\n",
    "    print(element['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al de links naar de auteur hebben gelukkig dezelfde text: \"(about)\", dus kunnen we deze toevoegen in de zoekfunctie.\n",
    "# Als we nu find_all gebruiken en daar doorheen loopen vinden we alle links terug:\n",
    "author_links = soup.find_all(\"a\", string=\"(about)\")\n",
    "\n",
    "for element in author_links:\n",
    "    print(element['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 3:\n",
    "\n",
    "Print de namen van alle auteurs op de startpagina van 'http://quotes.toscrape.com/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#antw3\">Antwoord Oefening 3</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oefening 4:\n",
    "\n",
    "Er zijn meerdere pagina's met quotes. We kunnen naar de volgende pagina gaan door op next() te klikken. Ook kunnen we zelf \n",
    "bedenken hoe het url pad eruit ziet en zo verschillende requests maken. Beide gedachtegangen kunnen we volgen als we meerdere pagina's willen scrapen. De eerste is het makkelijkst. \n",
    "\n",
    "Oefening: Scarape van de eerste 3 pagina's alle quotes en print deze. Kies zelf je methode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#antw4\">Antwoord Oefening 4</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zelf gaan scrapen\n",
    "\n",
    "Nu wordt het tijd om iets te gaan scrapen wat relevant is voor jouw eigen werk of gewoon je interesse heeft. Succes. Mocht je op den duur naar geavanceerdere toepassingen willen kijken (bijvoorbeeld hogere snelheid) van scrapen, dan is scrapy het package voor jou. Wil je hiermee aan de slag, kijk naar het volgende online tutorial: https://docs.scrapy.org/en/latest/intro/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antwoorden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"#opdr1\">Antwoord Oefening 1</a> <a name=\"antw1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even zeker weten dat de goede url gebruikt wordt\n",
    "url = 'http://web.mta.info/developers/turnstile.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") \n",
    "\n",
    "# zoek alle a tags (a' tags zijn voor de links)\n",
    "all_a_tags =  soup.findAll('a')\n",
    "\n",
    "# loop door de nummers die je wilt (a-tags zijn er veel te veel)\n",
    "for i in range(36,42): # i is 36,37,.. 41\n",
    "    one_a_tag = all_a_tags[i]\n",
    "    \n",
    "    # zoek de waarde van href (de relatieve link)\n",
    "    link = one_a_tag['href']\n",
    "    \n",
    "    # combineer de relative link met de basis link\n",
    "    download_url = 'http://web.mta.info/developers/'+ link\n",
    "    \n",
    "    # maak het verzoek te downloaden\n",
    "    urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:]) \n",
    "    \n",
    "    # voeg de pauze toe om de website niet te over belasten (en zelf niet geblokt te worden)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"#opdr2\">Antwoord Oefening 2</a> <a name=\"antw2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"#opdr3\">Antwoord Oefening 3</a> <a name=\"antw3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = soup.find_all('small', attrs={'class' :\"author\"})\n",
    "for author in authors:\n",
    "    print (author.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href=\"#opdr4\">Antwoord Oefening 4</a> <a name=\"antw4\"></a> - twee opties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Oplossing 1:\n",
    "\n",
    "# Door goed te kijken naar de opbouw van de URLs zie ik dat ze als volgt lopen\n",
    "#http://quotes.toscrape.com/page/1/\n",
    "#http://quotes.toscrape.com/page/2/\n",
    "# .... tot en met\n",
    "#http://quotes.toscrape.com/page/10/\n",
    "\n",
    "# Het is dus mogelijk om beurtlings die urls aan te roepen en te scrapen. \n",
    "start_url_format = \"http://quotes.toscrape.com/page/{}/\"\n",
    "\n",
    "for i in range(1,4): #getallen 1 tm 3\n",
    "    # First parse the desired url\n",
    "    url = start_url_format.format(i)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Second search for the quotes & print!\n",
    "    all_quotes_elements = soup.find_all('span', attrs={'class': 'text', 'itemprop':'text'})\n",
    "    for quote in all_quotes_elements:\n",
    "        print(quote.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Oplossing 2:\n",
    "\n",
    "# De opvolgende link kan ook gevonden worden door te zoeken by de tag die hoort bij de 'next' knop. \n",
    "# Het inspecteren van het element geeft de volgende tag: <a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>\n",
    "# Deze kunnen is relatief en kunnen we koppelen aan de basis url: \"http://quotes.toscrape.com\"\n",
    "\n",
    "def print_all_quotes(soup):\n",
    "    \"\"\"\n",
    "    This function takes the parsed response and looks for tag & attributes that\n",
    "    define the quote text on the quotes.toscrape webpage and prints these\n",
    "    \"\"\"\n",
    "    all_quotes_elements = soup.find_all('span', attrs={'class': 'text', 'itemprop':'text'})\n",
    "    for quote in all_quotes_elements:\n",
    "        print(quote.text)    \n",
    "\n",
    "        \n",
    "def scrape_next_page(soup, base_url):\n",
    "    \"\"\"\n",
    "    get link to second page and return the new parsed html.\n",
    "    \"\"\"\n",
    "    # find the right tag by looking at the more specific class next above the actual link\n",
    "    page_tag = soup.find(\"li\", attrs= {'class': 'next'})\n",
    "    \n",
    "    # within that look for the a tag and the href attribute\n",
    "    page_link = page_tag.find('a')['href']    \n",
    "    new_url = base_url + page_link\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    return  BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "\n",
    "# The script!\n",
    "\n",
    "base_url = \"http://quotes.toscrape.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#print the quotes for the first page\n",
    "print_all_quotes(soup)\n",
    "\n",
    "# find the link to next page and scrape it\n",
    "soup = scrape_next_page(soup, base_url)\n",
    "print_all_quotes(soup)\n",
    "\n",
    "# And again for page 3 (obviously, this could also be written in a for or while loop to scrape all pages)\n",
    "soup = scrape_next_page(soup, base_url)\n",
    "print_all_quotes(soup)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
